{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4e522324",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygame\n",
      "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pygame\n",
      "Successfully installed pygame-2.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install gym\n",
    "!pip install keras\n",
    "!pip install keras-rl2\n",
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4daea275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.animation\n",
    "from IPython.display import display, clear_output\n",
    "import random\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7eab60f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "code2action = {\n",
    "    0:np.array([0, 0, 0, 0]),\n",
    "    1:np.array([0, 0, 1, 0]),\n",
    "    2:np.array([0, 0, -1, 0]),\n",
    "    3:np.array([0, 0, 0, 1]),\n",
    "    4:np.array([0, 0, 0, -1]),\n",
    "    5:np.array([0, 0, 1, 1]),\n",
    "    6:np.array([0, 0, 1, -1]),\n",
    "    7:np.array([0, 0, -1, 1]),\n",
    "    8:np.array([0, 0, -1, -1])\n",
    "}\n",
    "\n",
    "class ShrinkingCircleEnv(Env):\n",
    "    width = height = d = 10\n",
    "    time = 60\n",
    "    terminated_penalty = 1\n",
    "    window = None\n",
    "    window_size = 512\n",
    "    clock = None\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 15}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.action_space = Discrete(9)\n",
    "        self.bounds = np.array([self.height, self.width, self.height, self.width]) - 1\n",
    "        self.observation_space = Box(np.zeros(4), self.bounds)\n",
    "        \n",
    "        \n",
    "        self.state = (np.random.random_sample(size=4)*self.bounds).astype(int)\n",
    "        \n",
    "        self.circle_radius = 0\n",
    "        \n",
    "        self.timestamp = self.time\n",
    "        \n",
    "        # for rendering\n",
    "        self.log_circles = []\n",
    "        self.log_hero = []\n",
    "\n",
    "        \n",
    "        \n",
    "    def get_reward(self):\n",
    "        circle, hero = self.state[:2], self.state[2:]\n",
    "        return - np.linalg.norm(hero - circle)\n",
    "        \n",
    "    def step(self, action):\n",
    "        new_state = self.state + code2action[action]\n",
    "        terminated = not ((0 <= new_state).all() and (new_state < self.bounds).all())\n",
    "        \n",
    "        self.timestamp -= 1 \n",
    "        self.truncated = self.timestamp<=0\n",
    "        \n",
    "        reward = self.get_reward()\n",
    "        \n",
    "        # move center of circle with probability 0.2\n",
    "        if np.random.binomial(1, 0.2):\n",
    "            self.state = np.hstack((\n",
    "                new_state[:2] + random.choice(list(code2action.values()))[2:],\n",
    "                new_state[2:]\n",
    "            ))\n",
    "        \n",
    "        # to stay on the playing field\n",
    "        self.state = np.clip(self.state, np.zeros(4), self.bounds)\n",
    "            \n",
    "        info = {}\n",
    "        return self.state, reward, self.truncated, info\n",
    "    \n",
    "    def render(self, mode):\n",
    "        self.render_mode = mode\n",
    "        circle, hero = self.state[:2], self.state[2:]\n",
    "        self.log_circles.append(circle)\n",
    "        self.log_hero.append(hero)\n",
    "        \n",
    "        return self._render_frame()\n",
    "    \n",
    "    # inspired by https://www.gymlibrary.dev/content/environment_creation/\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "        \n",
    "        \n",
    "        circle, hero = self.state[:2], self.state[2:]\n",
    "        \n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.d\n",
    "        )  # The size of a single grid square in pixels\n",
    "\n",
    "        # First we draw the hero\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * hero,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "        # Now we draw the circle\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            (0, 0, 255),\n",
    "            (circle + 0.5) * pix_square_size,\n",
    "            pix_square_size / 3,\n",
    "        )\n",
    "\n",
    "        # Finally, add some gridlines\n",
    "        for x in range(self.d + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=3,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=3,\n",
    "            )\n",
    "        \n",
    "        font = pygame.font.Font(None, 25)\n",
    "        text = font.render(f\"Time remaining: {str(self.timestamp)}\", True,(0, 0, 128))\n",
    "        text_rect = text.get_rect(center=(self.window_size/6, self.window_size/15))\n",
    "        \n",
    "        if self.render_mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            self.window.blit(text, text_rect)\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "        \n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        self.state = (np.random.random_sample(size=4)*self.bounds).astype(int)\n",
    "        self.circle_radius = 0 #min(self.height, self.width)\n",
    "        self.timestamp = self.time\n",
    "        \n",
    "        self.log_circles = []\n",
    "        self.log_hero = []\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        return self.state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8ccb4679",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ShrinkingCircleEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "775e7b8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "No available video device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m      9\u001b[0m     n_state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 10\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     score\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mreward\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Score:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(episode, score))\n",
      "Cell \u001b[0;32mIn[89], line 72\u001b[0m, in \u001b[0;36mShrinkingCircleEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_circles\u001b[38;5;241m.\u001b[39mappend(circle)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_hero\u001b[38;5;241m.\u001b[39mappend(hero)\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[89], line 78\u001b[0m, in \u001b[0;36mShrinkingCircleEnv._render_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     77\u001b[0m     pygame\u001b[38;5;241m.\u001b[39minit()\n\u001b[0;32m---> 78\u001b[0m     \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mset_mode((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size))\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31merror\u001b[0m: No available video device"
     ]
    }
   ],
   "source": [
    "episodes = 3\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        env.render('human')\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79157592",
   "metadata": {},
   "source": [
    "### Let's learn the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ab46b774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, CategoryEncoding\n",
    "from tensorflow.keras.optimizers.legacy import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "eb6312fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(states, n_actions=9):\n",
    "    model = Sequential() \n",
    "    model.add(keras.Input(shape=(1,states[0])))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Flatten()) \n",
    "    model.add(Dense(n_actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bb9b6a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "38ee33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "01bcc525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 20:43:09.763821: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_18/kernel/Assign' id:4321 op device:{requested: '', assigned: ''} def:{{{node dense_18/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_18/kernel, dense_18/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "model = get_model(n_states, n_actions)\n",
    "dqn = get_agent(model, n_actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8659b778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 20:00:38.557947: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_11/BiasAdd' id:2594 op device:{requested: '', assigned: ''} def:{{{node dense_11/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_11/MatMul, dense_11/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-24 20:00:38.637886: W tensorflow/c/c_api.cc:291] Operation '{name:'total_12/Assign' id:2839 op device:{requested: '', assigned: ''} def:{{{node total_12/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](total_12, total_12/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "    1/10000 [..............................] - ETA: 35:38 - reward: -3.6056"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 20:00:38.788645: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_11_1/BiasAdd' id:2710 op device:{requested: '', assigned: ''} def:{{{node dense_11_1/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_11_1/MatMul, dense_11_1/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-24 20:00:39.104717: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_15/AddN' id:2970 op device:{requested: '', assigned: ''} def:{{{node loss_15/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_15/mul, loss_15/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-24 20:00:39.202497: W tensorflow/c/c_api.cc:291] Operation '{name:'training_6/Adam/learning_rate/Assign' id:3135 op device:{requested: '', assigned: ''} def:{{{node training_6/Adam/learning_rate/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_6/Adam/learning_rate, training_6/Adam/learning_rate/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  510/10000 [>.............................] - ETA: 2:01 - reward: -4.2579done, took 6.800 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f85986bafe0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=20000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "35bed218",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/vladtom/miniconda3/lib/python3.10/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-02-24 20:08:00.256401: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_17/BiasAdd' id:3921 op device:{requested: '', assigned: ''} def:{{{node dense_17/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_17/MatMul, dense_17/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-24 20:08:00.397395: W tensorflow/c/c_api.cc:291] Operation '{name:'count_22/Assign' id:4191 op device:{requested: '', assigned: ''} def:{{{node count_22/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](count_22, count_22/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "No available video device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(scores\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode_reward\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/rl/core.py:352\u001b[0m, in \u001b[0;36mAgent.test\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    351\u001b[0m     observation, r, d, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mprocess_step(observation, r, d, info)\n\u001b[0;32m--> 352\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_action_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m info\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/rl/callbacks.py:98\u001b[0m, in \u001b[0;36mCallbackList.on_action_end\u001b[0;34m(self, action, logs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callable(\u001b[38;5;28mgetattr\u001b[39m(callback, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_action_end\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[0;32m---> 98\u001b[0m         \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_action_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/rl/callbacks.py:360\u001b[0m, in \u001b[0;36mVisualizer.on_action_end\u001b[0;34m(self, action, logs)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_action_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, action, logs):\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;124;03m\"\"\" Render environment at the end of each action \"\"\"\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[79], line 69\u001b[0m, in \u001b[0;36mShrinkingCircleEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_circles\u001b[38;5;241m.\u001b[39mappend(circle)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_hero\u001b[38;5;241m.\u001b[39mappend(hero)\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[79], line 75\u001b[0m, in \u001b[0;36mShrinkingCircleEnv._render_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     74\u001b[0m     pygame\u001b[38;5;241m.\u001b[39minit()\n\u001b[0;32m---> 75\u001b[0m     \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mset_mode((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size))\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31merror\u001b[0m: No available video device"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=10, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4971027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/25333732/matplotlib-animation-not-working-in-ipython-notebook-blank-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756b9e72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
