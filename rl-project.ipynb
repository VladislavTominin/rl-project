{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4e522324",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygame\n",
      "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pygame\n",
      "Successfully installed pygame-2.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install gym\n",
    "!pip install keras\n",
    "!pip install keras-rl2\n",
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4daea275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.animation\n",
    "from IPython.display import display, clear_output\n",
    "import random\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eab60f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "code2action = {\n",
    "    0:np.array([0, 0, 0, 0]),\n",
    "    1:np.array([0, 0, 1, 0]),\n",
    "    2:np.array([0, 0, -1, 0]),\n",
    "    3:np.array([0, 0, 0, 1]),\n",
    "    4:np.array([0, 0, 0, -1]),\n",
    "    5:np.array([0, 0, 1, 1]),\n",
    "    6:np.array([0, 0, 1, -1]),\n",
    "    7:np.array([0, 0, -1, 1]),\n",
    "    8:np.array([0, 0, -1, -1])\n",
    "}\n",
    "\n",
    "class ShrinkingCircleEnv(Env):\n",
    "    width = height = d = 10\n",
    "    time = 60\n",
    "    terminated_penalty = 1\n",
    "    window = None\n",
    "    window_size = 512\n",
    "    clock = None\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 15}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.action_space = Discrete(9)\n",
    "        self.bounds = np.array([self.height, self.width, self.height, self.width]) - 1\n",
    "        self.observation_space = Box(np.zeros(4), self.bounds)\n",
    "        \n",
    "        \n",
    "        self.state = (np.random.random_sample(size=4)*self.bounds).astype(int)\n",
    "        \n",
    "        self.circle_radius = 0\n",
    "        \n",
    "        self.timestamp = self.time\n",
    "        \n",
    "        # for rendering\n",
    "        self.log_circles = []\n",
    "        self.log_hero = []\n",
    "\n",
    "        \n",
    "        \n",
    "    def get_reward(self):\n",
    "        circle, hero = self.state[:2], self.state[2:]\n",
    "        return - np.linalg.norm(hero - circle)\n",
    "        \n",
    "    def step(self, action):\n",
    "        new_state = self.state + code2action[action]\n",
    "        terminated = not ((0 <= new_state).all() and (new_state < self.bounds).all())\n",
    "        \n",
    "        self.timestamp -= 1 \n",
    "        self.truncated = self.timestamp<=0\n",
    "        \n",
    "        reward = self.get_reward()\n",
    "        \n",
    "        # move center of circle with probability 0.2\n",
    "        if np.random.binomial(1, 0.2):\n",
    "            self.state = np.hstack((\n",
    "                new_state[:2] + random.choice(list(code2action.values()))[2:],\n",
    "                new_state[2:]\n",
    "            ))\n",
    "        \n",
    "        # to stay on the playing field\n",
    "        self.state = np.clip(self.state, np.zeros(4), self.bounds)\n",
    "            \n",
    "        info = {}\n",
    "        return self.state, reward, self.truncated, info\n",
    "    \n",
    "    def render(self, mode):\n",
    "        self.render_mode = mode\n",
    "        circle, hero = self.state[:2], self.state[2:]\n",
    "        self.log_circles.append(circle)\n",
    "        self.log_hero.append(hero)\n",
    "        \n",
    "        return self._render_frame()\n",
    "    \n",
    "    # inspired by https://www.gymlibrary.dev/content/environment_creation/\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "        \n",
    "        \n",
    "        circle, hero = self.state[:2], self.state[2:]\n",
    "        \n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.d\n",
    "        )  # The size of a single grid square in pixels\n",
    "\n",
    "        # First we draw the hero\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * hero,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "        # Now we draw the circle\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            (0, 0, 255),\n",
    "            (circle + 0.5) * pix_square_size,\n",
    "            pix_square_size / 3,\n",
    "        )\n",
    "\n",
    "        # Finally, add some gridlines\n",
    "        for x in range(self.d + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=3,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=3,\n",
    "            )\n",
    "        \n",
    "        font = pygame.font.Font(None, 25)\n",
    "        text = font.render(f\"Time remaining: {str(self.timestamp)}\", True,(0, 0, 128))\n",
    "        text_rect = text.get_rect(center=(self.window_size/6, self.window_size/15))\n",
    "        \n",
    "        if self.render_mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            self.window.blit(text, text_rect)\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "        \n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        self.state = (np.random.random_sample(size=4)*self.bounds).astype(int)\n",
    "        self.circle_radius = 0 #min(self.height, self.width)\n",
    "        self.timestamp = self.time\n",
    "        \n",
    "        self.log_circles = []\n",
    "        self.log_hero = []\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        return self.state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ccb4679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/vladtom/miniconda3/lib/python3.10/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "env = ShrinkingCircleEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "775e7b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-225.69925692169008\n",
      "Episode:2 Score:-287.00561184937595\n",
      "Episode:3 Score:-515.7849374229106\n"
     ]
    }
   ],
   "source": [
    "episodes = 3\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "#         env.render('human')\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79157592",
   "metadata": {},
   "source": [
    "### Let's learn the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab46b774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-25 00:35:01.669567: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-25 00:35:01.994562: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /homes/vladtom/miniconda3/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-02-25 00:35:01.994598: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-25 00:35:03.101111: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /homes/vladtom/miniconda3/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-02-25 00:35:03.101236: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /homes/vladtom/miniconda3/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-02-25 00:35:03.101246: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, CategoryEncoding\n",
    "from tensorflow.keras.optimizers.legacy import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb6312fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(states, n_actions=9, compact=False):\n",
    "    model = Sequential() \n",
    "    model.add(keras.Input(shape=(1,states[0])))\n",
    "    if not compact:\n",
    "        model.add(Dense(1024, activation='relu'))\n",
    "        model.add(Dense(100, activation='relu'))\n",
    "    model.add(Flatten()) \n",
    "    model.add(Dense(n_actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb9b6a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38ee33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01bcc525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/vladtom/miniconda3/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "2023-02-25 00:35:05.185222: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /homes/vladtom/miniconda3/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-02-25 00:35:05.185367: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /homes/vladtom/miniconda3/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-02-25 00:35:05.185460: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /homes/vladtom/miniconda3/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-02-25 00:35:05.242944: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /homes/vladtom/miniconda3/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-02-25 00:35:05.243055: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /homes/vladtom/miniconda3/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-02-25 00:35:05.243076: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-02-25 00:35:05.243681: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-25 00:35:05.264900: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "2023-02-25 00:35:05.288608: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_2_1/kernel/Assign' id:223 op device:{requested: '', assigned: ''} def:{{{node dense_2_1/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_2_1/kernel, dense_2_1/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /homes/vladtom/miniconda3/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "model = get_model(n_states, n_actions)\n",
    "dqn = get_agent(model, n_actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccdc95ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: -254.775, steps: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/vladtom/miniconda3/lib/python3.10/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-02-25 00:35:12.302804: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_2/BiasAdd' id:117 op device:{requested: '', assigned: ''} def:{{{node dense_2/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_2/MatMul, dense_2/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-25 00:35:12.326201: W tensorflow/c/c_api.cc:291] Operation '{name:'total_2/Assign' id:382 op device:{requested: '', assigned: ''} def:{{{node total_2/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](total_2, total_2/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2: reward: -515.735, steps: 60\n",
      "Episode 3: reward: -412.104, steps: 60\n",
      "Episode 4: reward: -411.073, steps: 60\n",
      "Episode 5: reward: -495.668, steps: 60\n",
      "Episode 6: reward: -413.858, steps: 60\n",
      "Episode 7: reward: -444.638, steps: 60\n",
      "Episode 8: reward: -481.286, steps: 60\n",
      "Episode 9: reward: -443.374, steps: 60\n",
      "Episode 10: reward: -194.967, steps: 60\n",
      "Episode 11: reward: -354.313, steps: 60\n",
      "Episode 12: reward: -288.513, steps: 60\n",
      "Episode 13: reward: -192.722, steps: 60\n",
      "Episode 14: reward: -370.973, steps: 60\n",
      "Episode 15: reward: -304.645, steps: 60\n",
      "Episode 16: reward: -400.049, steps: 60\n",
      "Episode 17: reward: -181.095, steps: 60\n",
      "Episode 18: reward: -458.646, steps: 60\n",
      "Episode 19: reward: -405.603, steps: 60\n",
      "Episode 20: reward: -329.403, steps: 60\n",
      "Episode 21: reward: -335.037, steps: 60\n",
      "Episode 22: reward: -183.614, steps: 60\n",
      "Episode 23: reward: -231.216, steps: 60\n",
      "Episode 24: reward: -276.245, steps: 60\n",
      "Episode 25: reward: -579.682, steps: 60\n",
      "Episode 26: reward: -545.108, steps: 60\n",
      "Episode 27: reward: -164.475, steps: 60\n",
      "Episode 28: reward: -457.129, steps: 60\n",
      "Episode 29: reward: -316.902, steps: 60\n",
      "Episode 30: reward: -392.068, steps: 60\n",
      "Episode 31: reward: -307.171, steps: 60\n",
      "Episode 32: reward: -185.116, steps: 60\n",
      "Episode 33: reward: -383.765, steps: 60\n",
      "Episode 34: reward: -372.478, steps: 60\n",
      "Episode 35: reward: -548.892, steps: 60\n",
      "Episode 36: reward: -522.249, steps: 60\n",
      "Episode 37: reward: -361.546, steps: 60\n",
      "Episode 38: reward: -270.830, steps: 60\n",
      "Episode 39: reward: -427.445, steps: 60\n",
      "Episode 40: reward: -267.276, steps: 60\n",
      "Episode 41: reward: -385.935, steps: 60\n",
      "Episode 42: reward: -459.652, steps: 60\n",
      "Episode 43: reward: -612.192, steps: 60\n",
      "Episode 44: reward: -278.216, steps: 60\n",
      "Episode 45: reward: -304.031, steps: 60\n",
      "Episode 46: reward: -302.611, steps: 60\n",
      "Episode 47: reward: -474.805, steps: 60\n",
      "Episode 48: reward: -239.783, steps: 60\n",
      "Episode 49: reward: -632.499, steps: 60\n",
      "Episode 50: reward: -311.507, steps: 60\n",
      "Episode 51: reward: -407.445, steps: 60\n",
      "Episode 52: reward: -499.631, steps: 60\n",
      "Episode 53: reward: -187.177, steps: 60\n",
      "Episode 54: reward: -398.909, steps: 60\n",
      "Episode 55: reward: -325.578, steps: 60\n",
      "Episode 56: reward: -333.197, steps: 60\n",
      "Episode 57: reward: -402.679, steps: 60\n",
      "Episode 58: reward: -454.965, steps: 60\n",
      "Episode 59: reward: -409.802, steps: 60\n",
      "Episode 60: reward: -190.339, steps: 60\n",
      "Episode 61: reward: -510.570, steps: 60\n",
      "Episode 62: reward: -140.415, steps: 60\n",
      "Episode 63: reward: -295.336, steps: 60\n",
      "Episode 64: reward: -323.682, steps: 60\n",
      "Episode 65: reward: -169.454, steps: 60\n",
      "Episode 66: reward: -227.364, steps: 60\n",
      "Episode 67: reward: -72.964, steps: 60\n",
      "Episode 68: reward: -188.168, steps: 60\n",
      "Episode 69: reward: -222.538, steps: 60\n",
      "Episode 70: reward: -582.384, steps: 60\n",
      "Episode 71: reward: -392.494, steps: 60\n",
      "Episode 72: reward: -367.001, steps: 60\n",
      "Episode 73: reward: -591.948, steps: 60\n",
      "Episode 74: reward: -254.005, steps: 60\n",
      "Episode 75: reward: -460.318, steps: 60\n",
      "Episode 76: reward: -304.927, steps: 60\n",
      "Episode 77: reward: -344.535, steps: 60\n",
      "Episode 78: reward: -554.086, steps: 60\n",
      "Episode 79: reward: -346.266, steps: 60\n",
      "Episode 80: reward: -172.948, steps: 60\n",
      "Episode 81: reward: -460.263, steps: 60\n",
      "Episode 82: reward: -234.419, steps: 60\n",
      "Episode 83: reward: -469.451, steps: 60\n",
      "Episode 84: reward: -250.119, steps: 60\n",
      "Episode 85: reward: -306.362, steps: 60\n",
      "Episode 86: reward: -428.314, steps: 60\n",
      "Episode 87: reward: -222.488, steps: 60\n",
      "Episode 88: reward: -486.000, steps: 60\n",
      "Episode 89: reward: -498.178, steps: 60\n",
      "Episode 90: reward: -99.337, steps: 60\n",
      "Episode 91: reward: -283.663, steps: 60\n",
      "Episode 92: reward: -405.988, steps: 60\n",
      "Episode 93: reward: -131.865, steps: 60\n",
      "Episode 94: reward: -416.200, steps: 60\n",
      "Episode 95: reward: -512.742, steps: 60\n",
      "Episode 96: reward: -170.058, steps: 60\n",
      "Episode 97: reward: -259.646, steps: 60\n",
      "Episode 98: reward: -327.299, steps: 60\n",
      "Episode 99: reward: -128.411, steps: 60\n",
      "Episode 100: reward: -411.480, steps: 60\n",
      "-351.1631803411789\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8659b778",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 20:00:38.557947: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_11/BiasAdd' id:2594 op device:{requested: '', assigned: ''} def:{{{node dense_11/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_11/MatMul, dense_11/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-24 20:00:38.637886: W tensorflow/c/c_api.cc:291] Operation '{name:'total_12/Assign' id:2839 op device:{requested: '', assigned: ''} def:{{{node total_12/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](total_12, total_12/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "    1/10000 [..............................] - ETA: 35:38 - reward: -3.6056"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 20:00:38.788645: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_11_1/BiasAdd' id:2710 op device:{requested: '', assigned: ''} def:{{{node dense_11_1/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_11_1/MatMul, dense_11_1/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-24 20:00:39.104717: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_15/AddN' id:2970 op device:{requested: '', assigned: ''} def:{{{node loss_15/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_15/mul, loss_15/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-24 20:00:39.202497: W tensorflow/c/c_api.cc:291] Operation '{name:'training_6/Adam/learning_rate/Assign' id:3135 op device:{requested: '', assigned: ''} def:{{{node training_6/Adam/learning_rate/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_6/Adam/learning_rate, training_6/Adam/learning_rate/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  510/10000 [>.............................] - ETA: 2:01 - reward: -4.2579done, took 6.800 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f85986bafe0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=20000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "35bed218",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/vladtom/miniconda3/lib/python3.10/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-02-24 20:08:00.256401: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_17/BiasAdd' id:3921 op device:{requested: '', assigned: ''} def:{{{node dense_17/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_17/MatMul, dense_17/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-24 20:08:00.397395: W tensorflow/c/c_api.cc:291] Operation '{name:'count_22/Assign' id:4191 op device:{requested: '', assigned: ''} def:{{{node count_22/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](count_22, count_22/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "No available video device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(scores\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode_reward\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/rl/core.py:352\u001b[0m, in \u001b[0;36mAgent.test\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    351\u001b[0m     observation, r, d, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mprocess_step(observation, r, d, info)\n\u001b[0;32m--> 352\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_action_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m info\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/rl/callbacks.py:98\u001b[0m, in \u001b[0;36mCallbackList.on_action_end\u001b[0;34m(self, action, logs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callable(\u001b[38;5;28mgetattr\u001b[39m(callback, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_action_end\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[0;32m---> 98\u001b[0m         \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_action_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/rl/callbacks.py:360\u001b[0m, in \u001b[0;36mVisualizer.on_action_end\u001b[0;34m(self, action, logs)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_action_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, action, logs):\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;124;03m\"\"\" Render environment at the end of each action \"\"\"\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[79], line 69\u001b[0m, in \u001b[0;36mShrinkingCircleEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_circles\u001b[38;5;241m.\u001b[39mappend(circle)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_hero\u001b[38;5;241m.\u001b[39mappend(hero)\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[79], line 75\u001b[0m, in \u001b[0;36mShrinkingCircleEnv._render_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     74\u001b[0m     pygame\u001b[38;5;241m.\u001b[39minit()\n\u001b[0;32m---> 75\u001b[0m     \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mset_mode((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size))\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31merror\u001b[0m: No available video device"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4971027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/25333732/matplotlib-animation-not-working-in-ipython-notebook-blank-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "756b9e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/vladtom/miniconda3/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "2023-02-25 01:32:44.383959: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_33_1/bias/Assign' id:8802 op device:{requested: '', assigned: ''} def:{{{node dense_33_1/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_33_1/bias, dense_33_1/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/vladtom/miniconda3/lib/python3.10/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-02-25 01:32:45.106672: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_33/BiasAdd' id:8781 op device:{requested: '', assigned: ''} def:{{{node dense_33/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_33/MatMul, dense_33/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-25 01:32:45.351855: W tensorflow/c/c_api.cc:291] Operation '{name:'count_44/Assign' id:8893 op device:{requested: '', assigned: ''} def:{{{node count_44/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](count_44, count_44/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "    1/10000 [..............................] - ETA: 1:35:52 - reward: -3.1623"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/vladtom/miniconda3/lib/python3.10/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "2023-02-25 01:32:45.690468: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_33_1/BiasAdd' id:8807 op device:{requested: '', assigned: ''} def:{{{node dense_33_1/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_33_1/MatMul, dense_33_1/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-25 01:32:46.054133: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_47/AddN' id:9019 op device:{requested: '', assigned: ''} def:{{{node loss_47/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_47/mul, loss_47/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-25 01:32:46.339893: W tensorflow/c/c_api.cc:291] Operation '{name:'training_20/Adam/iter/Assign' id:9146 op device:{requested: '', assigned: ''} def:{{{node training_20/Adam/iter/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_INT64, validate_shape=false](training_20/Adam/iter, training_20/Adam/iter/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  997/10000 [=>............................] - ETA: 1:30 - reward: -6.1817done, took 10.667 seconds\n",
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: -112.000, steps: 60\n",
      "Episode 2: reward: -294.498, steps: 60\n",
      "Episode 3: reward: -460.022, steps: 60\n",
      "Episode 4: reward: -262.560, steps: 60\n",
      "Episode 5: reward: -383.005, steps: 60\n",
      "Episode 6: reward: -277.662, steps: 60\n",
      "Episode 7: reward: -317.106, steps: 60\n",
      "Episode 8: reward: -488.024, steps: 60\n",
      "Episode 9: reward: -259.566, steps: 60\n",
      "Episode 10: reward: -518.755, steps: 60\n",
      "Episode 11: reward: -331.904, steps: 60\n",
      "Episode 12: reward: -329.018, steps: 60\n",
      "Episode 13: reward: -357.307, steps: 60\n",
      "Episode 14: reward: -279.401, steps: 60\n",
      "Episode 15: reward: -431.188, steps: 60\n",
      "Episode 16: reward: -288.292, steps: 60\n",
      "Episode 17: reward: -374.036, steps: 60\n",
      "Episode 18: reward: -416.121, steps: 60\n",
      "Episode 19: reward: -392.670, steps: 60\n",
      "Episode 20: reward: -295.328, steps: 60\n",
      "Episode 21: reward: -97.008, steps: 60\n",
      "Episode 22: reward: -406.642, steps: 60\n",
      "Episode 23: reward: -459.128, steps: 60\n",
      "Episode 24: reward: -297.017, steps: 60\n",
      "Episode 25: reward: -455.085, steps: 60\n",
      "Episode 26: reward: -244.506, steps: 60\n",
      "Episode 27: reward: -220.066, steps: 60\n",
      "Episode 28: reward: -422.553, steps: 60\n",
      "Episode 29: reward: -266.131, steps: 60\n",
      "Episode 30: reward: -313.457, steps: 60\n",
      "Episode 31: reward: -280.002, steps: 60\n",
      "Episode 32: reward: -286.880, steps: 60\n",
      "Episode 33: reward: -318.298, steps: 60\n",
      "Episode 34: reward: -403.121, steps: 60\n",
      "Episode 35: reward: -358.460, steps: 60\n",
      "Episode 36: reward: -490.236, steps: 60\n",
      "Episode 37: reward: -415.165, steps: 60\n",
      "Episode 38: reward: -162.046, steps: 60\n",
      "Episode 39: reward: -179.859, steps: 60\n",
      "Episode 40: reward: -286.423, steps: 60\n",
      "Episode 41: reward: -197.828, steps: 60\n",
      "Episode 42: reward: -402.576, steps: 60\n",
      "Episode 43: reward: -411.428, steps: 60\n",
      "Episode 44: reward: -386.779, steps: 60\n",
      "Episode 45: reward: -174.485, steps: 60\n",
      "Episode 46: reward: -47.107, steps: 60\n",
      "Episode 47: reward: -392.869, steps: 60\n",
      "Episode 48: reward: -141.356, steps: 60\n",
      "Episode 49: reward: -480.457, steps: 60\n",
      "Episode 50: reward: -226.485, steps: 60\n",
      "Episode 51: reward: -541.079, steps: 60\n",
      "Episode 52: reward: -99.944, steps: 60\n",
      "Episode 53: reward: -451.536, steps: 60\n",
      "Episode 54: reward: -113.822, steps: 60\n",
      "Episode 55: reward: -158.734, steps: 60\n",
      "Episode 56: reward: -314.988, steps: 60\n",
      "Episode 57: reward: -562.633, steps: 60\n",
      "Episode 58: reward: -395.056, steps: 60\n",
      "Episode 59: reward: -166.113, steps: 60\n",
      "Episode 60: reward: -277.203, steps: 60\n",
      "Episode 61: reward: -486.046, steps: 60\n",
      "Episode 62: reward: -240.164, steps: 60\n",
      "Episode 63: reward: -481.146, steps: 60\n",
      "Episode 64: reward: -382.467, steps: 60\n",
      "Episode 65: reward: -403.085, steps: 60\n",
      "Episode 66: reward: -279.271, steps: 60\n",
      "Episode 67: reward: -184.008, steps: 60\n",
      "Episode 68: reward: -184.398, steps: 60\n",
      "Episode 69: reward: -505.941, steps: 60\n",
      "Episode 70: reward: -182.969, steps: 60\n",
      "Episode 71: reward: -274.547, steps: 60\n",
      "Episode 72: reward: -372.360, steps: 60\n",
      "Episode 73: reward: -515.190, steps: 60\n",
      "Episode 74: reward: -246.676, steps: 60\n",
      "Episode 75: reward: -500.284, steps: 60\n",
      "Episode 76: reward: -219.276, steps: 60\n",
      "Episode 77: reward: -344.356, steps: 60\n",
      "Episode 78: reward: -319.607, steps: 60\n",
      "Episode 79: reward: -474.017, steps: 60\n",
      "Episode 80: reward: -476.678, steps: 60\n",
      "Episode 81: reward: -50.971, steps: 60\n",
      "Episode 82: reward: -533.645, steps: 60\n",
      "Episode 83: reward: -150.013, steps: 60\n",
      "Episode 84: reward: -326.786, steps: 60\n",
      "Episode 85: reward: -613.798, steps: 60\n",
      "Episode 86: reward: -420.517, steps: 60\n",
      "Episode 87: reward: -198.190, steps: 60\n",
      "Episode 88: reward: -409.348, steps: 60\n",
      "Episode 89: reward: -521.462, steps: 60\n",
      "Episode 90: reward: -404.724, steps: 60\n",
      "Episode 91: reward: -279.317, steps: 60\n",
      "Episode 92: reward: -552.418, steps: 60\n",
      "Episode 93: reward: -474.290, steps: 60\n",
      "Episode 94: reward: -192.373, steps: 60\n",
      "Episode 95: reward: -408.935, steps: 60\n",
      "Episode 96: reward: -423.436, steps: 60\n",
      "Episode 97: reward: -158.608, steps: 60\n",
      "Episode 98: reward: -240.616, steps: 60\n",
      "Episode 99: reward: -571.470, steps: 60\n",
      "Episode 100: reward: -210.063, steps: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-25 01:33:01.371434: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_34_1/bias/Assign' id:9305 op device:{requested: '', assigned: ''} def:{{{node dense_34_1/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_34_1/bias, dense_34_1/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-25 01:33:02.119396: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_34/BiasAdd' id:9284 op device:{requested: '', assigned: ''} def:{{{node dense_34/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_34/MatMul, dense_34/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-25 01:33:02.408084: W tensorflow/c/c_api.cc:291] Operation '{name:'count_48/Assign' id:9396 op device:{requested: '', assigned: ''} def:{{{node count_48/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](count_48, count_48/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "    1/10000 [..............................] - ETA: 1:47:17 - reward: -3.6056"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-25 01:33:02.772604: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_34_1/BiasAdd' id:9310 op device:{requested: '', assigned: ''} def:{{{node dense_34_1/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_34_1/MatMul, dense_34_1/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-25 01:33:03.168100: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_51/AddN' id:9522 op device:{requested: '', assigned: ''} def:{{{node loss_51/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_51/mul, loss_51/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-25 01:33:03.473145: W tensorflow/c/c_api.cc:291] Operation '{name:'training_22/Adam/iter/Assign' id:9649 op device:{requested: '', assigned: ''} def:{{{node training_22/Adam/iter/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_INT64, validate_shape=false](training_22/Adam/iter, training_22/Adam/iter/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4994/10000 [=============>................] - ETA: 44s - reward: -4.9769done, took 45.257 seconds\n",
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: -256.288, steps: 60\n",
      "Episode 2: reward: -249.211, steps: 60\n",
      "Episode 3: reward: -183.886, steps: 60\n",
      "Episode 4: reward: -129.235, steps: 60\n",
      "Episode 5: reward: -508.187, steps: 60\n",
      "Episode 6: reward: -452.140, steps: 60\n",
      "Episode 7: reward: -196.511, steps: 60\n",
      "Episode 8: reward: -365.287, steps: 60\n",
      "Episode 9: reward: -267.529, steps: 60\n",
      "Episode 10: reward: -418.087, steps: 60\n",
      "Episode 11: reward: -155.106, steps: 60\n",
      "Episode 12: reward: -257.382, steps: 60\n",
      "Episode 13: reward: -323.810, steps: 60\n",
      "Episode 14: reward: -163.233, steps: 60\n",
      "Episode 15: reward: -294.320, steps: 60\n",
      "Episode 16: reward: -184.933, steps: 60\n",
      "Episode 17: reward: -108.507, steps: 60\n",
      "Episode 18: reward: -352.627, steps: 60\n",
      "Episode 19: reward: -170.124, steps: 60\n",
      "Episode 20: reward: -160.156, steps: 60\n",
      "Episode 21: reward: -189.340, steps: 60\n",
      "Episode 22: reward: -544.382, steps: 60\n",
      "Episode 23: reward: -273.472, steps: 60\n",
      "Episode 24: reward: -329.808, steps: 60\n",
      "Episode 25: reward: -201.453, steps: 60\n",
      "Episode 26: reward: -217.357, steps: 60\n",
      "Episode 27: reward: -438.558, steps: 60\n",
      "Episode 28: reward: -223.642, steps: 60\n",
      "Episode 29: reward: -264.354, steps: 60\n",
      "Episode 30: reward: -414.611, steps: 60\n",
      "Episode 31: reward: -481.071, steps: 60\n",
      "Episode 32: reward: -171.062, steps: 60\n",
      "Episode 33: reward: -162.919, steps: 60\n",
      "Episode 34: reward: -259.549, steps: 60\n",
      "Episode 35: reward: -468.538, steps: 60\n",
      "Episode 36: reward: -163.914, steps: 60\n",
      "Episode 37: reward: -288.130, steps: 60\n",
      "Episode 38: reward: -291.981, steps: 60\n",
      "Episode 39: reward: -155.751, steps: 60\n",
      "Episode 40: reward: -259.092, steps: 60\n",
      "Episode 41: reward: -244.845, steps: 60\n",
      "Episode 42: reward: -394.693, steps: 60\n",
      "Episode 43: reward: -240.398, steps: 60\n",
      "Episode 44: reward: -256.810, steps: 60\n",
      "Episode 45: reward: -73.946, steps: 60\n",
      "Episode 46: reward: -227.243, steps: 60\n",
      "Episode 47: reward: -330.665, steps: 60\n",
      "Episode 48: reward: -264.672, steps: 60\n",
      "Episode 49: reward: -341.261, steps: 60\n",
      "Episode 50: reward: -188.334, steps: 60\n",
      "Episode 51: reward: -147.075, steps: 60\n",
      "Episode 52: reward: -404.222, steps: 60\n",
      "Episode 53: reward: -258.037, steps: 60\n",
      "Episode 54: reward: -270.697, steps: 60\n",
      "Episode 55: reward: -252.434, steps: 60\n",
      "Episode 56: reward: -294.102, steps: 60\n",
      "Episode 57: reward: -267.225, steps: 60\n",
      "Episode 58: reward: -277.792, steps: 60\n",
      "Episode 59: reward: -161.142, steps: 60\n",
      "Episode 60: reward: -282.940, steps: 60\n",
      "Episode 61: reward: -266.399, steps: 60\n",
      "Episode 62: reward: -230.351, steps: 60\n",
      "Episode 63: reward: -619.631, steps: 60\n",
      "Episode 64: reward: -117.187, steps: 60\n",
      "Episode 65: reward: -242.380, steps: 60\n",
      "Episode 66: reward: -400.725, steps: 60\n",
      "Episode 67: reward: -234.127, steps: 60\n",
      "Episode 68: reward: -204.431, steps: 60\n",
      "Episode 69: reward: -368.383, steps: 60\n",
      "Episode 70: reward: -420.445, steps: 60\n",
      "Episode 71: reward: -208.556, steps: 60\n",
      "Episode 72: reward: -361.964, steps: 60\n",
      "Episode 73: reward: -96.598, steps: 60\n",
      "Episode 74: reward: -279.613, steps: 60\n",
      "Episode 75: reward: -297.186, steps: 60\n",
      "Episode 76: reward: -232.120, steps: 60\n",
      "Episode 77: reward: -319.099, steps: 60\n",
      "Episode 78: reward: -105.164, steps: 60\n",
      "Episode 79: reward: -144.550, steps: 60\n",
      "Episode 80: reward: -499.773, steps: 60\n",
      "Episode 81: reward: -154.499, steps: 60\n",
      "Episode 82: reward: -480.817, steps: 60\n",
      "Episode 83: reward: -344.392, steps: 60\n",
      "Episode 84: reward: -360.309, steps: 60\n",
      "Episode 85: reward: -519.524, steps: 60\n",
      "Episode 86: reward: -77.096, steps: 60\n",
      "Episode 87: reward: -365.241, steps: 60\n",
      "Episode 88: reward: -273.277, steps: 60\n",
      "Episode 89: reward: -385.986, steps: 60\n",
      "Episode 90: reward: -169.977, steps: 60\n",
      "Episode 91: reward: -433.497, steps: 60\n",
      "Episode 92: reward: -416.202, steps: 60\n",
      "Episode 93: reward: -276.372, steps: 60\n",
      "Episode 94: reward: -350.044, steps: 60\n",
      "Episode 95: reward: -195.377, steps: 60\n",
      "Episode 96: reward: -416.581, steps: 60\n",
      "Episode 97: reward: -337.143, steps: 60\n",
      "Episode 98: reward: -380.039, steps: 60\n",
      "Episode 99: reward: -136.492, steps: 60\n",
      "Episode 100: reward: -244.153, steps: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-25 01:33:52.907691: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_35_1/bias/Assign' id:9808 op device:{requested: '', assigned: ''} def:{{{node dense_35_1/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_35_1/bias, dense_35_1/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-25 01:33:53.702633: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_35/BiasAdd' id:9787 op device:{requested: '', assigned: ''} def:{{{node dense_35/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_35/MatMul, dense_35/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-25 01:33:54.010626: W tensorflow/c/c_api.cc:291] Operation '{name:'count_52/Assign' id:9899 op device:{requested: '', assigned: ''} def:{{{node count_52/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](count_52, count_52/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "    1/10000 [..............................] - ETA: 1:55:47 - reward: -2.2361"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-25 01:33:54.406555: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_35_1/BiasAdd' id:9813 op device:{requested: '', assigned: ''} def:{{{node dense_35_1/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_35_1/MatMul, dense_35_1/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-25 01:33:54.821821: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_55/AddN' id:10025 op device:{requested: '', assigned: ''} def:{{{node loss_55/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_55/mul, loss_55/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-25 01:33:55.154010: W tensorflow/c/c_api.cc:291] Operation '{name:'training_24/Adam/iter/Assign' id:10152 op device:{requested: '', assigned: ''} def:{{{node training_24/Adam/iter/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_INT64, validate_shape=false](training_24/Adam/iter, training_24/Adam/iter/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 87s 9ms/step - reward: -4.9927\n",
      "done, took 86.693 seconds\n",
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: -139.351, steps: 60\n",
      "Episode 2: reward: -332.048, steps: 60\n",
      "Episode 3: reward: -92.006, steps: 60\n",
      "Episode 4: reward: -313.736, steps: 60\n",
      "Episode 5: reward: -334.136, steps: 60\n",
      "Episode 6: reward: -281.411, steps: 60\n",
      "Episode 7: reward: -310.364, steps: 60\n",
      "Episode 8: reward: -282.835, steps: 60\n",
      "Episode 9: reward: -208.411, steps: 60\n",
      "Episode 10: reward: -315.280, steps: 60\n",
      "Episode 11: reward: -171.734, steps: 60\n",
      "Episode 12: reward: -254.972, steps: 60\n",
      "Episode 13: reward: -247.485, steps: 60\n",
      "Episode 14: reward: -279.719, steps: 60\n",
      "Episode 15: reward: -480.154, steps: 60\n",
      "Episode 16: reward: -131.490, steps: 60\n",
      "Episode 17: reward: -387.048, steps: 60\n",
      "Episode 18: reward: -479.140, steps: 60\n",
      "Episode 19: reward: -167.459, steps: 60\n",
      "Episode 20: reward: -405.495, steps: 60\n",
      "Episode 21: reward: -398.035, steps: 60\n",
      "Episode 22: reward: -472.366, steps: 60\n",
      "Episode 23: reward: -146.537, steps: 60\n",
      "Episode 24: reward: -262.397, steps: 60\n",
      "Episode 25: reward: -379.410, steps: 60\n",
      "Episode 26: reward: -374.412, steps: 60\n",
      "Episode 27: reward: -337.557, steps: 60\n",
      "Episode 28: reward: -231.437, steps: 60\n",
      "Episode 29: reward: -440.255, steps: 60\n",
      "Episode 30: reward: -322.970, steps: 60\n",
      "Episode 31: reward: -381.711, steps: 60\n",
      "Episode 32: reward: -62.759, steps: 60\n",
      "Episode 33: reward: -512.947, steps: 60\n",
      "Episode 34: reward: -283.784, steps: 60\n",
      "Episode 35: reward: -156.792, steps: 60\n",
      "Episode 36: reward: -337.028, steps: 60\n",
      "Episode 37: reward: -420.739, steps: 60\n",
      "Episode 38: reward: -301.961, steps: 60\n",
      "Episode 39: reward: -130.513, steps: 60\n",
      "Episode 40: reward: -163.990, steps: 60\n",
      "Episode 41: reward: -255.168, steps: 60\n",
      "Episode 42: reward: -449.144, steps: 60\n",
      "Episode 43: reward: -257.138, steps: 60\n",
      "Episode 44: reward: -369.080, steps: 60\n",
      "Episode 45: reward: -314.017, steps: 60\n",
      "Episode 46: reward: -231.563, steps: 60\n",
      "Episode 47: reward: -177.903, steps: 60\n",
      "Episode 48: reward: -379.546, steps: 60\n",
      "Episode 49: reward: -360.092, steps: 60\n",
      "Episode 50: reward: -156.137, steps: 60\n",
      "Episode 51: reward: -208.356, steps: 60\n",
      "Episode 52: reward: -406.648, steps: 60\n",
      "Episode 53: reward: -443.822, steps: 60\n",
      "Episode 54: reward: -267.566, steps: 60\n",
      "Episode 55: reward: -286.284, steps: 60\n",
      "Episode 56: reward: -106.630, steps: 60\n",
      "Episode 57: reward: -154.498, steps: 60\n",
      "Episode 58: reward: -155.512, steps: 60\n",
      "Episode 59: reward: -376.465, steps: 60\n",
      "Episode 60: reward: -455.790, steps: 60\n",
      "Episode 61: reward: -501.192, steps: 60\n",
      "Episode 62: reward: -219.515, steps: 60\n",
      "Episode 63: reward: -252.066, steps: 60\n",
      "Episode 64: reward: -228.701, steps: 60\n",
      "Episode 65: reward: -202.969, steps: 60\n",
      "Episode 66: reward: -198.998, steps: 60\n",
      "Episode 67: reward: -428.231, steps: 60\n",
      "Episode 68: reward: -233.098, steps: 60\n",
      "Episode 69: reward: -193.397, steps: 60\n",
      "Episode 70: reward: -235.494, steps: 60\n",
      "Episode 71: reward: -200.596, steps: 60\n",
      "Episode 72: reward: -165.212, steps: 60\n",
      "Episode 73: reward: -326.759, steps: 60\n",
      "Episode 74: reward: -137.745, steps: 60\n",
      "Episode 75: reward: -231.650, steps: 60\n",
      "Episode 76: reward: -351.858, steps: 60\n",
      "Episode 77: reward: -371.692, steps: 60\n",
      "Episode 78: reward: -540.157, steps: 60\n",
      "Episode 79: reward: -540.206, steps: 60\n",
      "Episode 80: reward: -436.376, steps: 60\n",
      "Episode 81: reward: -308.120, steps: 60\n",
      "Episode 82: reward: -137.209, steps: 60\n",
      "Episode 83: reward: -149.503, steps: 60\n",
      "Episode 84: reward: -152.423, steps: 60\n",
      "Episode 85: reward: -426.341, steps: 60\n",
      "Episode 86: reward: -368.428, steps: 60\n",
      "Episode 87: reward: -55.213, steps: 60\n",
      "Episode 88: reward: -298.734, steps: 60\n",
      "Episode 89: reward: -300.206, steps: 60\n",
      "Episode 90: reward: -458.638, steps: 60\n",
      "Episode 91: reward: -238.928, steps: 60\n",
      "Episode 92: reward: -325.171, steps: 60\n",
      "Episode 93: reward: -443.864, steps: 60\n",
      "Episode 94: reward: -337.340, steps: 60\n",
      "Episode 95: reward: -263.592, steps: 60\n",
      "Episode 96: reward: -218.308, steps: 60\n",
      "Episode 97: reward: -540.093, steps: 60\n",
      "Episode 98: reward: -426.202, steps: 60\n",
      "Episode 99: reward: -111.627, steps: 60\n",
      "Episode 100: reward: -212.041, steps: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-25 01:35:25.864848: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_36_1/bias/Assign' id:10311 op device:{requested: '', assigned: ''} def:{{{node dense_36_1/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_36_1/bias, dense_36_1/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 15000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-25 01:35:26.724006: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_36/BiasAdd' id:10290 op device:{requested: '', assigned: ''} def:{{{node dense_36/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_36/MatMul, dense_36/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-25 01:35:27.058612: W tensorflow/c/c_api.cc:291] Operation '{name:'count_57/Assign' id:10412 op device:{requested: '', assigned: ''} def:{{{node count_57/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](count_57, count_57/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "    1/10000 [..............................] - ETA: 2:08:18 - reward: -6.3246"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-25 01:35:27.503688: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_36_1/BiasAdd' id:10316 op device:{requested: '', assigned: ''} def:{{{node dense_36_1/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_36_1/MatMul, dense_36_1/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-25 01:35:27.951155: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_59/AddN' id:10528 op device:{requested: '', assigned: ''} def:{{{node loss_59/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_59/mul, loss_59/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-25 01:35:28.288045: W tensorflow/c/c_api.cc:291] Operation '{name:'training_26/Adam/dense_36/bias/v/Assign' id:10695 op device:{requested: '', assigned: ''} def:{{{node training_26/Adam/dense_36/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_26/Adam/dense_36/bias/v, training_26/Adam/dense_36/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 87s 9ms/step - reward: -5.1495\n",
      "166 episodes - episode_reward: -308.798 [-574.726, -67.997] - loss: 21.084 - mae: 35.311 - mean_q: -37.765\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      " 4997/10000 [=============>................] - ETA: 43s - reward: -5.0467done, took 130.045 seconds\n",
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: -318.966, steps: 60\n",
      "Episode 2: reward: -361.902, steps: 60\n",
      "Episode 3: reward: -273.567, steps: 60\n",
      "Episode 4: reward: -255.433, steps: 60\n",
      "Episode 5: reward: -179.865, steps: 60\n",
      "Episode 6: reward: -428.808, steps: 60\n",
      "Episode 7: reward: -216.606, steps: 60\n",
      "Episode 8: reward: -184.364, steps: 60\n",
      "Episode 9: reward: -164.103, steps: 60\n",
      "Episode 10: reward: -435.612, steps: 60\n",
      "Episode 11: reward: -349.788, steps: 60\n",
      "Episode 12: reward: -390.604, steps: 60\n",
      "Episode 13: reward: -290.370, steps: 60\n",
      "Episode 14: reward: -212.644, steps: 60\n",
      "Episode 15: reward: -213.657, steps: 60\n",
      "Episode 16: reward: -402.042, steps: 60\n",
      "Episode 17: reward: -220.162, steps: 60\n",
      "Episode 18: reward: -337.833, steps: 60\n",
      "Episode 19: reward: -351.669, steps: 60\n",
      "Episode 20: reward: -358.305, steps: 60\n",
      "Episode 21: reward: -434.217, steps: 60\n",
      "Episode 22: reward: -303.679, steps: 60\n",
      "Episode 23: reward: -388.822, steps: 60\n",
      "Episode 24: reward: -110.859, steps: 60\n",
      "Episode 25: reward: -419.241, steps: 60\n",
      "Episode 26: reward: -562.036, steps: 60\n",
      "Episode 27: reward: -424.156, steps: 60\n",
      "Episode 28: reward: -413.550, steps: 60\n",
      "Episode 29: reward: -173.026, steps: 60\n",
      "Episode 30: reward: -456.197, steps: 60\n",
      "Episode 31: reward: -129.948, steps: 60\n",
      "Episode 32: reward: -428.641, steps: 60\n",
      "Episode 33: reward: -147.995, steps: 60\n",
      "Episode 34: reward: -247.076, steps: 60\n",
      "Episode 35: reward: -496.608, steps: 60\n",
      "Episode 36: reward: -370.134, steps: 60\n",
      "Episode 37: reward: -185.276, steps: 60\n",
      "Episode 38: reward: -62.057, steps: 60\n",
      "Episode 39: reward: -468.701, steps: 60\n",
      "Episode 40: reward: -297.575, steps: 60\n",
      "Episode 41: reward: -380.849, steps: 60\n",
      "Episode 42: reward: -364.555, steps: 60\n",
      "Episode 43: reward: -502.221, steps: 60\n",
      "Episode 44: reward: -433.587, steps: 60\n",
      "Episode 45: reward: -438.133, steps: 60\n",
      "Episode 46: reward: -361.981, steps: 60\n",
      "Episode 47: reward: -370.307, steps: 60\n",
      "Episode 48: reward: -405.591, steps: 60\n",
      "Episode 49: reward: -202.520, steps: 60\n",
      "Episode 50: reward: -157.008, steps: 60\n",
      "Episode 51: reward: -377.907, steps: 60\n",
      "Episode 52: reward: -432.594, steps: 60\n",
      "Episode 53: reward: -364.667, steps: 60\n",
      "Episode 54: reward: -440.435, steps: 60\n",
      "Episode 55: reward: -122.351, steps: 60\n",
      "Episode 56: reward: -132.652, steps: 60\n",
      "Episode 57: reward: -305.527, steps: 60\n",
      "Episode 58: reward: -219.189, steps: 60\n",
      "Episode 59: reward: -334.031, steps: 60\n",
      "Episode 60: reward: -318.555, steps: 60\n",
      "Episode 61: reward: -402.287, steps: 60\n",
      "Episode 62: reward: -361.610, steps: 60\n",
      "Episode 63: reward: -164.249, steps: 60\n",
      "Episode 64: reward: -164.535, steps: 60\n",
      "Episode 65: reward: -397.488, steps: 60\n",
      "Episode 66: reward: -433.466, steps: 60\n",
      "Episode 67: reward: -259.018, steps: 60\n",
      "Episode 68: reward: -212.473, steps: 60\n",
      "Episode 69: reward: -275.623, steps: 60\n",
      "Episode 70: reward: -230.064, steps: 60\n",
      "Episode 71: reward: -221.622, steps: 60\n",
      "Episode 72: reward: -380.160, steps: 60\n",
      "Episode 73: reward: -437.575, steps: 60\n",
      "Episode 74: reward: -95.577, steps: 60\n",
      "Episode 75: reward: -370.437, steps: 60\n",
      "Episode 76: reward: -133.455, steps: 60\n",
      "Episode 77: reward: -319.773, steps: 60\n",
      "Episode 78: reward: -449.670, steps: 60\n",
      "Episode 79: reward: -272.792, steps: 60\n",
      "Episode 80: reward: -223.544, steps: 60\n",
      "Episode 81: reward: -248.210, steps: 60\n",
      "Episode 82: reward: -284.412, steps: 60\n",
      "Episode 83: reward: -228.528, steps: 60\n",
      "Episode 84: reward: -236.679, steps: 60\n",
      "Episode 85: reward: -358.687, steps: 60\n",
      "Episode 86: reward: -268.432, steps: 60\n",
      "Episode 87: reward: -455.734, steps: 60\n",
      "Episode 88: reward: -287.743, steps: 60\n",
      "Episode 89: reward: -394.713, steps: 60\n",
      "Episode 90: reward: -157.167, steps: 60\n",
      "Episode 91: reward: -443.641, steps: 60\n",
      "Episode 92: reward: -185.948, steps: 60\n",
      "Episode 93: reward: -393.556, steps: 60\n",
      "Episode 94: reward: -216.371, steps: 60\n",
      "Episode 95: reward: -376.491, steps: 60\n",
      "Episode 96: reward: -368.050, steps: 60\n",
      "Episode 97: reward: -366.666, steps: 60\n",
      "Episode 98: reward: -407.669, steps: 60\n",
      "Episode 99: reward: -368.096, steps: 60\n",
      "Episode 100: reward: -312.550, steps: 60\n"
     ]
    }
   ],
   "source": [
    "scores_log = {}\n",
    "for n_steps in [1000, 5000, 10000, 15000]:\n",
    "    model = get_model(n_states, n_actions, compact=True)\n",
    "    dqn = get_agent(model, n_actions)\n",
    "    dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "    dqn.fit(env, nb_steps=n_steps, visualize=False, verbose=1)\n",
    "    scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "    scores_log[n_steps] = np.mean(scores.history['episode_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73482a1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1000: -308.8078342331893,\n",
       " 5000: -143.2580174170538,\n",
       " 10000: -155.78609127837154,\n",
       " 15000: -149.57824776422228,\n",
       " 20000: -228.72297844445487,\n",
       " 25000: -204.61390880288442,\n",
       " 35000: -159.23455552933467,\n",
       " 45000: -157.56615966927995}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4348e4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1000: -333.8648923996682,\n",
       " 5000: -281.39773406147486,\n",
       " 10000: -293.11126799543183,\n",
       " 15000: -312.95515351060504}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8796d37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
